<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Random Forest â€” Full Example with Explanation</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f4f4f4;
      padding: 2rem;
      color: #333;
      line-height: 1.7;
    }
    h1, h2, h3 {
      color: #2e7d32;
    }
    .section {
      background-color: #fff;
      border-radius: 8px;
      padding: 1.5rem;
      margin-bottom: 2rem;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 1rem;
    }
    th, td {
      border: 1px solid #ccc;
      padding: 0.5rem;
      text-align: center;
    }
    th {
      background-color: #e0e0e0;
    }
    code {
      background-color: #eee;
      padding: 2px 6px;
      border-radius: 4px;
      font-weight: bold;
    }
    .leaf {
      color: #1565c0;
      font-weight: bold;
    }
  </style>
</head>
<body>
  <h1>ğŸŒ³ Random Forest â€” Full Example with Explanation</h1>

  <div class="section">
    <h2>ğŸ” What is Random Forest?</h2>
    <p>Random Forest is an ensemble learning method that builds many decision trees and combines their outputs by majority vote (classification) or average (regression). It's especially powerful because it reduces overfitting, handles high-dimensional data, and learns complex patterns well.</p>
  </div>

  <div class="section">
    <h2>ğŸ§¾ Step 1: Our Dataset</h2>
    <p>We have 5 features (words): <strong>great, bad, normal, amazing, awful</strong>. We'll use 4 sentences, and manually assign estimated TF-IDF values.</p>
    <table>
      <tr>
        <th>Sentence</th><th>great</th><th>bad</th><th>normal</th><th>amazing</th><th>awful</th><th>Label</th>
      </tr>
      <tr><td>"great and amazing"</td><td>0.8</td><td>0.0</td><td>0.0</td><td>0.7</td><td>0.0</td><td>1</td></tr>
      <tr><td>"this is bad and awful"</td><td>0.0</td><td>0.9</td><td>0.0</td><td>0.0</td><td>0.8</td><td>0</td></tr>
      <tr><td>"it was normal"</td><td>0.0</td><td>0.1</td><td>0.9</td><td>0.0</td><td>0.0</td><td>0</td></tr>
      <tr><td>"great but a bit normal"</td><td>0.6</td><td>0.0</td><td>0.6</td><td>0.0</td><td>0.0</td><td>1</td></tr>
    </table>
  </div>

  <div class="section">
    <h2>ğŸŒ² Step 2: Building Tree 1 (with multiple nodes)</h2>
    <p>We create a bootstrap sample: sentences 1, 2, 3, and again 1. The tree selects features "bad" and "amazing" for the root node.</p>
    <h3>â¡ï¸ Node 1: Split on <code>bad > 0.5</code></h3>
    <ul>
      <li>Left (bad â‰¤ 0.5): S1, S3, S1 â†’ Labels = [1, 0, 1]</li>
      <li>Right (bad > 0.5): S2 â†’ Label = 0 â†’ Pure leaf</li>
    </ul>
    <h3>â¡ï¸ Node 2 (left): Split on <code>amazing > 0.5</code></h3>
    <ul>
      <li>Left: S3 â†’ Label = 0 â†’ Pure</li>
      <li>Right: S1, S1 â†’ Labels = [1, 1] â†’ Pure</li>
    </ul>
    <h3>âœ… Tree 1 Structure</h3>
    <pre>
[bad > 0.5]
â”œâ”€â”€ Yes â†’ Leaf: Predict 0
â””â”€â”€ No â†’ [amazing > 0.5]
        â”œâ”€â”€ Yes â†’ Leaf: Predict 1
        â””â”€â”€ No â†’ Leaf: Predict 0
    </pre>
  </div>

  <div class="section">
    <h2>ğŸŒ² Step 3: Building Tree 2 (with multiple nodes)</h2>
    <p>Bootstrap sample: sentences 2, 3, 4, and 4. Chosen features: "normal" and "great".</p>
    <h3>â¡ï¸ Node 1: Split on <code>normal > 0.5</code></h3>
    <ul>
      <li>Left: S2 â†’ Label = 0 â†’ Pure</li>
      <li>Right: S3, S4, S4 â†’ Labels = [0, 1, 1]</li>
    </ul>
    <h3>â¡ï¸ Node 2 (right): Split on <code>great > 0.5</code></h3>
    <ul>
      <li>Left: S3 â†’ Label = 0</li>
      <li>Right: S4, S4 â†’ Labels = [1, 1]</li>
    </ul>
    <h3>âœ… Tree 2 Structure</h3>
    <pre>
[normal > 0.5]
â”œâ”€â”€ No â†’ Leaf: Predict 0
â””â”€â”€ Yes â†’ [great > 0.5]
        â”œâ”€â”€ Yes â†’ Leaf: Predict 1
        â””â”€â”€ No â†’ Leaf: Predict 0
    </pre>
  </div>

  <div class="section">
    <h2>ğŸ§ª Step 4: Prediction for a New Sentence</h2>
    <p><strong>Sentence:</strong> "This was normal but also amazing and a bit great"</p>
    <p>Estimated TF-IDF:</p>
    <table>
      <tr><th>great</th><th>bad</th><th>normal</th><th>amazing</th><th>awful</th></tr>
      <tr><td>0.6</td><td>0.0</td><td>0.8</td><td>0.6</td><td>0.0</td></tr>
    </table>
    <h3>ğŸŒ³ Tree 1 Decision:</h3>
    <ul>
      <li>bad = 0.0 â†’ go left</li>
      <li>amazing = 0.6 â†’ go right â†’ Leaf = Predict 1</li>
    </ul>
    <h3>ğŸŒ² Tree 2 Decision:</h3>
    <ul>
      <li>normal = 0.8 â†’ go right</li>
      <li>great = 0.6 â†’ go right â†’ Leaf = Predict 1</li>
    </ul>
    <h3>âœ… Final Random Forest Prediction:</h3>
    <p>Tree 1 â†’ Positive<br>Tree 2 â†’ Positive<br><strong>ğŸ‰ Final Prediction: Positive (1)</strong></p>
  </div>

  <div class="section">
    <h2>ğŸ“š Mathematical Concepts</h2>
    <h3>ğŸ§  Gini Impurity</h3>
    <p>At each split, we compute how pure the node is using:</p>
    <pre>Gini = 1 - Î£(páµ¢Â²)</pre>
    <p>Where páµ¢ is the proportion of samples of class i (e.g. positive or negative).</p>
    <h3>ğŸ¯ Goal of Split</h3>
    <p>Try many possible rules (feature + threshold), and select the one that gives the <strong>maximum Gini gain</strong>:</p>
    <pre>
Î”G = G(parent) - [ (n_left/n) * G(left) + (n_right/n) * G(right) ]
    </pre>
    <p>This is repeated <strong>at each node</strong> until:</p>
    <ul>
      <li>Max depth is reached</li>
      <li>Nodes are pure</li>
      <li>Not enough samples to split</li>
    </ul>
    <h3>âœ… Final Prediction</h3>
    <p>Each tree gives 0 or 1 â†’ Random Forest returns the <strong>majority vote</strong>.</p>
  </div>
</body>
</html>
